{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ca72469-9df7-435a-9a75-981f07342771",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n|            sentence|              source|\n+--------------------+--------------------+\n|\"The specific epi...|' in many cases i...|\n|Adult and pediatr...|    pages_articles24|\n|\"He received the ...|    pages_articles24|\n|He competed for G...|    pages_articles24|\n|Despite an increa...|    pages_articles24|\n+--------------------+--------------------+\nonly showing top 5 rows\n\nNumber of distinct sentences in large.csv.gz: 389639\n+----------+\n|word_count|\n+----------+\n|      4571|\n|      2499|\n|       562|\n|       528|\n|       426|\n|       413|\n|       382|\n|       381|\n|       348|\n|       335|\n+----------+\n\nAverage number of bigrams per sentence: 18.0365\n+------+-----+\n|bigram|count|\n+------+-----+\n|of the|76374|\n+------+-----+\n\nMaximum number of words in any idiom: 9\nNumber of idioms from MAGPIE found in Wikipedia: 354\nTop bigrams ranked 2500 to 2509 (excluding MAGPIE idioms):\nannounced in (188)\nas director (188)\nfew years (188)\nfor being (188)\ngroup and (188)\nsea level (188)\nthe imperial (188)\nthe northeast (188)\nthe research (188)\nthe situation (188)\n"
     ]
    }
   ],
   "source": [
    "# Q1: “Calculate the number of different sentences in the dataset.”\n",
    "# Step 1: Load Wikipedia data (large.csv.gz)\n",
    "wiki_df = spark.read.csv(\"/FileStore/tables/large_csv.gz\", header=True, inferSchema=True)\n",
    "# Check the first few lines to confirm the format\n",
    "wiki_df.show(5)\n",
    "# Clean up empty lines or invalid sentences\n",
    "wiki_df = wiki_df.filter(wiki_df[\"sentence\"].isNotNull() & (wiki_df[\"sentence\"] != \"\"))\n",
    "# Step 2: Select the sentence column + remove duplicates\n",
    "distinct_sentences = wiki_df.select(\"sentence\").distinct()\n",
    "# Step 3: Count the number of different sentences\n",
    "num_distinct_sentences = distinct_sentences.count()\n",
    "# Step 4: Output the results\n",
    "print(f\"Number of distinct sentences in large.csv.gz: {num_distinct_sentences}\")\n",
    "\n",
    "\n",
    "# Q2: “Longest Sentences by Word Count.”\n",
    "from pyspark.sql.functions import split, size\n",
    "# Count the words\n",
    "wiki_with_word_count = wiki_df.withColumn(\"word_count\", size(split(wiki_df[\"sentence\"], \" \")))\n",
    "# Get the number of words in the top 10 longest sentences\n",
    "top10_word_counts = wiki_with_word_count.orderBy(\"word_count\", ascending=False).select(\"word_count\").limit(10)\n",
    "top10_word_counts.show()\n",
    "\n",
    "# Q3: “Calculate the average number of bigrams per sentence.”\n",
    "from pyspark.sql.functions import split, size, when, col, sum as spark_sum\n",
    "# Step 1: Loading data + cleaning\n",
    "wiki_df = spark.read.csv(\"/FileStore/tables/large_csv.gz\", header=True, inferSchema=True)\n",
    "wiki_df = wiki_df.filter(wiki_df[\"sentence\"].isNotNull() & (wiki_df[\"sentence\"] != \"\"))\n",
    "# Step 2: Tokenize and count words\n",
    "wiki_df = wiki_df.withColumn(\"word_count\", size(split(col(\"sentence\"), \" \")))\n",
    "# Step 3: Calculate the number of bigrams per sentence\n",
    "wiki_df = wiki_df.withColumn(\"bigram_count\", when(col(\"word_count\") >= 2, col(\"word_count\") - 1).otherwise(0))\n",
    "# Step 4: Find the total number of bigrams\n",
    "total_bigrams = wiki_df.agg(spark_sum(\"bigram_count\")).collect()[0][0]\n",
    "# Step 5: Find the total number of sentences\n",
    "total_sentences = wiki_df.count()\n",
    "# Step 6: average number of bigrams\n",
    "avg_bigrams = total_bigrams / total_sentences\n",
    "print(f\"Average number of bigrams per sentence: {avg_bigrams:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Q4: “Calculate the average number of bigrams per sentence.”\n",
    "from pyspark.sql.functions import split, regexp_replace, explode, col\n",
    "from pyspark.ml.feature import NGram\n",
    "# Step 1: Loading and cleaning data\n",
    "wiki_df = spark.read.csv(\"/FileStore/tables/large_csv.gz\", header=True, inferSchema=True)\n",
    "wiki_df = wiki_df.filter(wiki_df[\"sentence\"].isNotNull() & (wiki_df[\"sentence\"] != \"\"))\n",
    "# Step 2: Remove punctuation (keep only letters, numbers, and spaces)\n",
    "wiki_clean = wiki_df.withColumn(\"clean_sentence\", regexp_replace(col(\"sentence\"), r\"[^\\w\\s]\", \"\"))\n",
    "# Step 3: Word segmentation (based on clean_sentence）\n",
    "wiki_tokenized = wiki_clean.withColumn(\"words\", split(col(\"clean_sentence\"), \" \"))\n",
    "# Step 4: Generating bigrams using NGram\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"bigrams\")\n",
    "wiki_bigrams_df = ngram.transform(wiki_tokenized)\n",
    "# Step 5: Expand bigrams into one row per column\n",
    "exploded_bigrams = wiki_bigrams_df.select(explode(col(\"bigrams\")).alias(\"bigram\"))\n",
    "# Step 6: Count frequencies & get the most common bigrams\n",
    "most_common_bigram = exploded_bigrams.groupBy(\"bigram\").count().orderBy(col(\"count\").desc()).limit(1)\n",
    "# Step 7: Output\n",
    "most_common_bigram.show(truncate=False)\n",
    "\n",
    "\n",
    "# Q5: “How many idioms occur in the Wikipedia data?”\n",
    "# Find the maximum number of words in the MAGPIE dataset\n",
    "from pyspark.sql.functions import size, split\n",
    "# Extract the idiom field from the MAGPIE dataset\n",
    "magpie = spark.read.json(\"/FileStore/tables/MAGPIE_unfiltered.jsonl\")\n",
    "idioms = magpie.select(\"idiom\").dropna().dropDuplicates()\n",
    "# Remove punctuation\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "idioms = idioms.withColumn(\"clean_idiom\", regexp_replace(\"idiom\", r\"[^\\w\\s]\", \"\"))\n",
    "# Split idiom into a list of words\n",
    "idioms = idioms.withColumn(\"word_count\", size(split(\"clean_idiom\", \" \")))\n",
    "# Find the maximum number of words\n",
    "max_len = idioms.agg({\"word_count\": \"max\"}).collect()[0][0]\n",
    "print(f\"Maximum number of words in any idiom: {max_len}\")\n",
    "\n",
    "\n",
    "# MAIN BODY\n",
    "from pyspark.sql.functions import col, split, explode, regexp_replace, lower\n",
    "from pyspark.ml.feature import NGram\n",
    "# Step 1: Loading and cleaning Wikipedia sentences\n",
    "wiki_df = spark.read.csv(\"/FileStore/tables/large_csv.gz\", header=True, inferSchema=True)\n",
    "wiki_df = wiki_df.filter(wiki_df[\"sentence\"].isNotNull() & (wiki_df[\"sentence\"] != \"\"))\n",
    "wiki_df = wiki_df.withColumn(\"clean_sentence\", regexp_replace(col(\"sentence\"), r\"[^\\w\\s]\", \"\"))\n",
    "wiki_df = wiki_df.withColumn(\"words\", split(col(\"clean_sentence\"), \" \"))\n",
    "# Step 2: Load and clean the MAGPIE idioms\n",
    "magpie_df = spark.read.json(\"/FileStore/tables/MAGPIE_unfiltered.jsonl\")\n",
    "idioms = magpie_df.select(\"idiom\").dropna().dropDuplicates()\n",
    "idioms = idioms.withColumn(\"idiom_clean\", regexp_replace(col(\"idiom\"), r\"[^\\w\\s]\", \"\"))\n",
    "idioms = idioms.withColumn(\"idiom_clean\", lower(col(\"idiom_clean\")))\n",
    "# Step 3: Matches all idioms from 2 to 9-grams\n",
    "matched_ngrams = None\n",
    "\n",
    "for n in range(2, 10):  # From 2 to 9\n",
    "    ngrammer = NGram(n=n, inputCol=\"words\", outputCol=\"ngrams\")\n",
    "    ngram_df = ngrammer.transform(wiki_df)\n",
    "    exploded = ngram_df.select(explode(col(\"ngrams\")).alias(\"ngram\"))\n",
    "    \n",
    "    # Unified format (remove punctuation, convert to lowercase)\n",
    "    exploded = exploded.withColumn(\"ngram\", lower(regexp_replace(col(\"ngram\"), r\"[^\\w\\s]\", \"\")))\n",
    "    \n",
    "    # Inner join matching idiom\n",
    "    matched = exploded.join(idioms, exploded[\"ngram\"] == idioms[\"idiom_clean\"])\n",
    "    \n",
    "    # Merge multiple matches of n\n",
    "    matched_ngrams = matched_ngrams.union(matched) if matched_ngrams else matched\n",
    "\n",
    "# Step 4: Count the number of idioms that appear (remove duplicates)\n",
    "idiom_count = matched_ngrams.select(\"idiom_clean\").distinct().count()\n",
    "print(f\"Number of idioms from MAGPIE found in Wikipedia: {idiom_count}\")\n",
    "\n",
    "# Q6: ““Print out the 10 bigrams starting from rank 2500 (ranked by frequency descending), skipping any that appear in MAGPIE. For ties, use alphabetical order.”\n",
    "from pyspark.sql.functions import split, regexp_replace, explode, col, lower, count as spark_count\n",
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "# Step 1: Loading and cleaning Wikipedia sentences\n",
    "wiki_df = spark.read.csv(\"/FileStore/tables/large_csv.gz\", header=True, inferSchema=True)\n",
    "wiki_df = wiki_df.filter(wiki_df[\"sentence\"].isNotNull() & (wiki_df[\"sentence\"] != \"\"))\n",
    "wiki_df = wiki_df.withColumn(\"clean_sentence\", regexp_replace(col(\"sentence\"), r\"[^\\w\\s]\", \"\"))\n",
    "wiki_df = wiki_df.withColumn(\"words\", split(col(\"clean_sentence\"), \" \"))\n",
    "\n",
    "# Step 2: Generate bigrams from Wikipedia\n",
    "ngrammer = NGram(n=2, inputCol=\"words\", outputCol=\"bigrams\")\n",
    "wiki_bigrams_df = ngrammer.transform(wiki_df)\n",
    "wiki_bigrams = wiki_bigrams_df.select(explode(col(\"bigrams\")).alias(\"bigram\"))\n",
    "wiki_bigrams = wiki_bigrams.withColumn(\"bigram\", lower(regexp_replace(col(\"bigram\"), r\"[^\\w\\s]\", \"\")))\n",
    "\n",
    "# Step 3: Count the frequency of bigram occurrences\n",
    "wiki_bigram_freq = wiki_bigrams.groupBy(\"bigram\").agg(spark_count(\"*\").alias(\"freq\"))\n",
    "\n",
    "# Step 4: Extract all bigrams in MAGPIE idioms (only generate n=2)\n",
    "magpie = spark.read.json(\"/FileStore/tables/MAGPIE_unfiltered.jsonl\")\n",
    "idioms = magpie.select(\"idiom\").dropna().dropDuplicates()\n",
    "idioms = idioms.withColumn(\"clean_idiom\", regexp_replace(lower(col(\"idiom\")), r\"[^\\w\\s]\", \"\"))\n",
    "idioms = idioms.withColumn(\"words\", split(col(\"clean_idiom\"), \" \"))\n",
    "magpie_bigrams = NGram(n=2, inputCol=\"words\", outputCol=\"bigrams\").transform(idioms)\n",
    "magpie_bigram_list = magpie_bigrams.select(explode(col(\"bigrams\")).alias(\"magpie_bigram\"))\n",
    "magpie_bigram_list = magpie_bigram_list.withColumn(\"magpie_bigram\", lower(regexp_replace(col(\"magpie_bigram\"), r\"[^\\w\\s]\", \"\")))\n",
    "\n",
    "# Step 5: Remove bigrams from MAGPIE\n",
    "filtered_wiki_bigrams = wiki_bigram_freq.join(\n",
    "    magpie_bigram_list,\n",
    "    wiki_bigram_freq[\"bigram\"] == magpie_bigram_list[\"magpie_bigram\"],\n",
    "    how=\"left_anti\"  # 只保留 wiki 中不在 MAGPIE 中的 bigram\n",
    ")\n",
    "\n",
    "# Step 6: Sort and take the 10 items ranked 2500~2509\n",
    "# (Frequency descending + alphabetical ascending)\n",
    "result = filtered_wiki_bigrams.orderBy(col(\"freq\").desc(), col(\"bigram\").asc()).limit(2510).tail(10)\n",
    "\n",
    "# Step 7: Printing Results\n",
    "print(\"Top bigrams ranked 2500 to 2509 (excluding MAGPIE idioms):\")\n",
    "for row in result:\n",
    "    print(f\"{row['bigram']} ({row['freq']})\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "INF6032_Assignment_Analysis",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}